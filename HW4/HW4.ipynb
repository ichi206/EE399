{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "Gerald I. Nakata\n",
    "\n",
    "**Github: https://github.com/ichi206/EE399/tree/main/HW4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Block\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# PCA & MNIST imports\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X=np.arange(0,31)\n",
    "Y=np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,\n",
    "40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i] Three conv layer feed forward neural network\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, inSize=1, hid1Size=10, hid2Size=10, outSize=1):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inSize, hid1Size)\n",
    "        self.fc2 = nn.Linear(hid1Size, hid2Size)\n",
    "        self.fc3 = nn.Linear(hid2Size, outSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=1511.5543\n",
      "Epoch 10: Loss=1003.8231\n",
      "Epoch 20: Loss=675.8133\n",
      "Epoch 30: Loss=456.8490\n",
      "Epoch 40: Loss=310.6668\n",
      "Epoch 50: Loss=213.0742\n",
      "Epoch 60: Loss=147.9206\n",
      "Epoch 70: Loss=104.4236\n",
      "Epoch 80: Loss=75.3846\n",
      "Epoch 90: Loss=55.9980\n",
      "Epoch 100: Loss=43.0553\n",
      "Epoch 110: Loss=34.4147\n",
      "Epoch 120: Loss=28.6461\n",
      "Epoch 130: Loss=24.7950\n",
      "Epoch 140: Loss=22.2240\n",
      "Epoch 150: Loss=20.5075\n",
      "Epoch 160: Loss=19.3616\n",
      "Epoch 170: Loss=18.5966\n",
      "Epoch 180: Loss=18.0858\n",
      "Epoch 190: Loss=17.7448\n",
      "Test loss: 181.2215\n"
     ]
    }
   ],
   "source": [
    "# [ii] Use first 20 pts as training and next 10 as test\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "# Convert arrays to tensors\n",
    "Xtrain = torch.from_numpy(X[:20]).float().reshape(-1, input_size)\n",
    "Ytrain = torch.from_numpy(Y[:20]).float().reshape(-1, output_size)\n",
    "Xtest = torch.from_numpy(X[-10:]).float().reshape(-1, input_size)\n",
    "Ytest = torch.from_numpy(Y[-10:]).float().reshape(-1, output_size)\n",
    "\n",
    "model = FFNN(inSize=1, hid1Size=20, hid2Size=5, outSize=1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(Xtrain)\n",
    "    loss = criterion(outputs, Ytrain)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch % 10 == 0):\n",
    "        print(f\"Epoch {epoch}: Loss={loss.item():.4f}\")\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Ypred = model(Xtest)\n",
    "\n",
    "test_loss = criterion(Ypred, Ytest)\n",
    "print(f\"Test loss: {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=1242.3337\n",
      "Epoch 10: Loss=129.1992\n",
      "Epoch 20: Loss=6.5838\n",
      "Test loss: 245.9613\n"
     ]
    }
   ],
   "source": [
    "# [iii] Use first 10 pts as training and next 10 as test\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "# Convert arrays to tensors\n",
    "Xtrain2 = torch.from_numpy(X[:10]).float().reshape(-1, input_size)\n",
    "Ytrain2 = torch.from_numpy(Y[:10]).float().reshape(-1, output_size)\n",
    "\n",
    "model2 = FFNN(inSize=1, hid1Size=20, hid2Size=5, outSize=1)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.01)\n",
    "\n",
    "# Train\n",
    "num_epochs2 = 30\n",
    "for epoch in range(num_epochs2):\n",
    "    outputs2 = model2(Xtrain2)\n",
    "    loss2 = criterion(outputs2, Ytrain2)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer2.zero_grad()\n",
    "    loss2.backward()\n",
    "    optimizer2.step()\n",
    "    if (epoch % 10 == 0):\n",
    "        print(f\"Epoch {epoch}: Loss={loss2.item():.4f}\")\n",
    "\n",
    "# Test\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    Ypred = model2(Xtest)\n",
    "\n",
    "test_loss = criterion(Ypred, Ytest)\n",
    "print(f\"Test loss: {test_loss.item():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[iv] Compare the fit models in HW1 to the neural nets.\n",
    "\n",
    "The test loss of the neural nets is worse than even the 19th degree polynomial in HW1, which paired with the comparison to the test loss demonstrates that neural nets have a proclivity to overfit to data rather quickly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ginak\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# [i] Compute first 20 PCA modes of MNIST data\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, Y = mnist.data / 255., mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 PCA Modes:\n",
      "[[-2.34759338e-18 -6.29140696e-19  1.23041352e-19 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-3.03168679e-17 -1.54113960e-18 -1.32578379e-18 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.85638963e-17 -1.64115091e-17  1.55878996e-18 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " ...\n",
      " [-1.73564523e-17 -4.62190910e-17  3.58883296e-19 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-8.05014655e-17 -3.36285756e-17  4.92940537e-17 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-8.06369294e-17  2.27895735e-17  3.40166631e-17 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]]\n",
      "Explained Variance: [0.09746116 0.07155445 0.06149531 0.05403385 0.04888934 0.04305227\n",
      " 0.03278262 0.02889642 0.02758364 0.0234214  0.02106688 0.02037553\n",
      " 0.01707061 0.01694019 0.01583378 0.01486337 0.01319264 0.01278985\n",
      " 0.01187208 0.01152689]\n"
     ]
    }
   ],
   "source": [
    "# Perform PCA with 20 components\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(X)\n",
    "#X_pca = pca.fit(X)\n",
    "print(\"First 20 PCA Modes:\")\n",
    "print(pca.components_)\n",
    "print(\"Explained Variance: \" + str(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ginak\\AppData\\Local\\Temp\\ipykernel_31404\\2794084576.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = (mnist['target'].values).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# [ii] Train a feed forward neural net on the MNIST data to classify digits. Compare results to LSTM, SVM, and decision tree classifiers.\n",
    "Y = (mnist['target'].values).astype(np.int)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "Y_train = torch.from_numpy(Y_train).long()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "Y_test = torch.from_numpy(Y_test).long()\n",
    "\n",
    "# Data loaders\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FFNN for MNIST data\n",
    "class FFNNM(nn.Module):\n",
    "    def __init__(self, inSize=784, hid1Size=256, hid2Size=128, outSize=10):\n",
    "        super(FFNNM, self).__init__()\n",
    "        self.fc1 = nn.Linear(inSize, hid1Size)\n",
    "        self.fc2 = nn.Linear(hid1Size, hid2Size)\n",
    "        self.fc3 = nn.Linear(hid2Size, outSize)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 2.294\n",
      "Epoch 2 loss: 2.213\n",
      "Epoch 3 loss: 1.975\n",
      "Epoch 4 loss: 1.830\n",
      "Epoch 5 loss: 1.742\n",
      "Epoch 6 loss: 1.669\n",
      "Epoch 7 loss: 1.630\n",
      "Epoch 8 loss: 1.589\n",
      "Epoch 9 loss: 1.571\n",
      "Epoch 10 loss: 1.560\n",
      "Epoch 11 loss: 1.553\n",
      "Epoch 12 loss: 1.547\n",
      "Epoch 13 loss: 1.543\n",
      "Epoch 14 loss: 1.539\n",
      "Epoch 15 loss: 1.535\n",
      "Accuracy of the net on the 14016 test images: 93.20 %\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "device = torch.device('cpu')\n",
    "model = FFNNM().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Train\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(\"Epoch %d loss: %.3f\" % (epoch + 1, total_loss / len(train_loader)))\n",
    "\n",
    "# Test\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, lables = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == lables).sum().item()\n",
    "print(\"Accuracy of the net on the %d test images: %.2f %%\" % (len(test_loader) * 64, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28).astype('float32') / 255\n",
    "\n",
    "# Convert the labels to categorical one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 25s 47ms/step - loss: 0.5307 - accuracy: 0.8280 - val_loss: 0.1821 - val_accuracy: 0.9427\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.1478 - accuracy: 0.9553 - val_loss: 0.1274 - val_accuracy: 0.9599\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 30s 65ms/step - loss: 0.1047 - accuracy: 0.9682 - val_loss: 0.0876 - val_accuracy: 0.9722\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 44s 93ms/step - loss: 0.0789 - accuracy: 0.9761 - val_loss: 0.0704 - val_accuracy: 0.9775\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 45s 95ms/step - loss: 0.0658 - accuracy: 0.9798 - val_loss: 0.0663 - val_accuracy: 0.9790\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 43s 92ms/step - loss: 0.0524 - accuracy: 0.9839 - val_loss: 0.0706 - val_accuracy: 0.9782\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 45s 95ms/step - loss: 0.0432 - accuracy: 0.9868 - val_loss: 0.0585 - val_accuracy: 0.9809\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0420 - accuracy: 0.9868 - val_loss: 0.0521 - val_accuracy: 0.9829\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 24s 51ms/step - loss: 0.0338 - accuracy: 0.9892 - val_loss: 0.0596 - val_accuracy: 0.9807\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 24s 51ms/step - loss: 0.0323 - accuracy: 0.9898 - val_loss: 0.0604 - val_accuracy: 0.9832\n",
      "Test loss: 0.060447126626968384\n",
      "Test accuracy: 0.9832000136375427\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(28, 28)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
